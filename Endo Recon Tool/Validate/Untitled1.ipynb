{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7396045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing single sheet 'Sheet1' from file 'MEMBER DETAILS--8_18_2024, 12_40_55 PM.xlsx'\n",
      "All required columns are present in file 'MEMBER DETAILS--8_18_2024, 12_40_55 PM.xlsx'.\n",
      "Processing single sheet 'ABHIPROD' from file 'MD_RN_UPGRADE_ABHIPROD_2-81-23-0001567-000_1723788251264.xlsx'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Master Policy Number</th>\n",
       "      <th>Combi Reference Number</th>\n",
       "      <th>Group Policy Number</th>\n",
       "      <th>Group Policy Holder name</th>\n",
       "      <th>Product Code</th>\n",
       "      <th>GPA Rater Plan</th>\n",
       "      <th>FAMILY_POLICY_NUMBER</th>\n",
       "      <th>Insured Member Name</th>\n",
       "      <th>Policy Start Date</th>\n",
       "      <th>Policy End Date</th>\n",
       "      <th>ABHI Client ID</th>\n",
       "      <th>Endorsement Reference Number</th>\n",
       "      <th>Employee Number</th>\n",
       "      <th>Member Entry Date</th>\n",
       "      <th>Member Exit Date</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Date of Birth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Annual Premium without ST /GST</th>\n",
       "      <th>GHI /GPA Sum Insured</th>\n",
       "      <th>CGST 9 %</th>\n",
       "      <th>SGST 9 %</th>\n",
       "      <th>IGST 18%</th>\n",
       "      <th>Total Charged Premium including ST /GST</th>\n",
       "      <th>Status  (Active /Inactive)</th>\n",
       "      <th>Category</th>\n",
       "      <th>Application Number entry 1</th>\n",
       "      <th>Partner Reference Number 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-81-23-0001567-000</td>\n",
       "      <td>IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...</td>\n",
       "      <td>5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GHI-81-23-5928542-000</td>\n",
       "      <td>DEVISHI  TOPKHANE</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-01-19 23:59:59</td>\n",
       "      <td>PT98606809</td>\n",
       "      <td>2-81-23-0001567-000-DE-010</td>\n",
       "      <td>101479</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-01-19 23:59:59</td>\n",
       "      <td>Son</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>CATEGORY 2</td>\n",
       "      <td>101479</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-81-23-0001567-000</td>\n",
       "      <td>IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...</td>\n",
       "      <td>5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GHI-81-23-5928542-000</td>\n",
       "      <td>KUHIKA  TOPKHANE</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-01-19 23:59:59</td>\n",
       "      <td>PT98606798</td>\n",
       "      <td>2-81-23-0001567-000-DE-010</td>\n",
       "      <td>101479</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-01-19 23:59:59</td>\n",
       "      <td>Daughter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>CATEGORY 2</td>\n",
       "      <td>101479</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-81-23-0001567-000</td>\n",
       "      <td>IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...</td>\n",
       "      <td>5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GHI-81-23-5928542-000</td>\n",
       "      <td>PRIYA  TOPKHANE</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-01-19 23:59:59</td>\n",
       "      <td>PT98606803</td>\n",
       "      <td>2-81-23-0001567-000-DE-010</td>\n",
       "      <td>101479</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-01-19 23:59:59</td>\n",
       "      <td>Spouse</td>\n",
       "      <td>19/02/1983</td>\n",
       "      <td>40.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>CATEGORY 2</td>\n",
       "      <td>101479</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-81-23-0001567-000</td>\n",
       "      <td>IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...</td>\n",
       "      <td>5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GHI-81-23-5928542-000</td>\n",
       "      <td>ROHIT DINKAR TOPKHANE</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-01-19 23:59:59</td>\n",
       "      <td>PT98606791</td>\n",
       "      <td>2-81-23-0001567-000-DE-010</td>\n",
       "      <td>101479</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-01-19 23:59:59</td>\n",
       "      <td>Self</td>\n",
       "      <td>11/08/1974</td>\n",
       "      <td>49.0</td>\n",
       "      <td>M</td>\n",
       "      <td>6414.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577.31</td>\n",
       "      <td>577.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7569.15</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>CATEGORY 2</td>\n",
       "      <td>101479</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-81-23-0001567-000</td>\n",
       "      <td>IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...</td>\n",
       "      <td>5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GHI-81-23-5928554-000</td>\n",
       "      <td>RAMCHANDRA DADA THENGAL</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-02-12 23:59:59</td>\n",
       "      <td>PT98606686</td>\n",
       "      <td>2-81-23-0001567-000-DE-013</td>\n",
       "      <td>101389</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2024-02-12 23:59:59</td>\n",
       "      <td>Spouse</td>\n",
       "      <td>03/02/1991</td>\n",
       "      <td>32.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>CATEGORY 2</td>\n",
       "      <td>101389</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Master Policy Number  Combi Reference Number  Group Policy Number  \\\n",
       "0                   NaN                     NaN  2-81-23-0001567-000   \n",
       "1                   NaN                     NaN  2-81-23-0001567-000   \n",
       "2                   NaN                     NaN  2-81-23-0001567-000   \n",
       "3                   NaN                     NaN  2-81-23-0001567-000   \n",
       "4                   NaN                     NaN  2-81-23-0001567-000   \n",
       "\n",
       "                            Group Policy Holder name  Product Code  \\\n",
       "0  IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...          5211   \n",
       "1  IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...          5211   \n",
       "2  IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...          5211   \n",
       "3  IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...          5211   \n",
       "4  IDEA LAKE INFORMATION TECHNOLOGIES PRIVATE LIM...          5211   \n",
       "\n",
       "   GPA Rater Plan   FAMILY_POLICY_NUMBER      Insured Member Name  \\\n",
       "0             NaN  GHI-81-23-5928542-000        DEVISHI  TOPKHANE   \n",
       "1             NaN  GHI-81-23-5928542-000         KUHIKA  TOPKHANE   \n",
       "2             NaN  GHI-81-23-5928542-000          PRIYA  TOPKHANE   \n",
       "3             NaN  GHI-81-23-5928542-000    ROHIT DINKAR TOPKHANE   \n",
       "4             NaN  GHI-81-23-5928554-000  RAMCHANDRA DADA THENGAL   \n",
       "\n",
       "  Policy Start Date     Policy End Date ABHI Client ID  \\\n",
       "0        2023-10-12 2024-01-19 23:59:59     PT98606809   \n",
       "1        2023-10-12 2024-01-19 23:59:59     PT98606798   \n",
       "2        2023-10-12 2024-01-19 23:59:59     PT98606803   \n",
       "3        2023-10-12 2024-01-19 23:59:59     PT98606791   \n",
       "4        2023-10-12 2024-02-12 23:59:59     PT98606686   \n",
       "\n",
       "  Endorsement Reference Number  Employee Number Member Entry Date  \\\n",
       "0   2-81-23-0001567-000-DE-010           101479        2023-10-12   \n",
       "1   2-81-23-0001567-000-DE-010           101479        2023-10-12   \n",
       "2   2-81-23-0001567-000-DE-010           101479        2023-10-12   \n",
       "3   2-81-23-0001567-000-DE-010           101479        2023-10-12   \n",
       "4   2-81-23-0001567-000-DE-013           101389        2023-10-12   \n",
       "\n",
       "     Member Exit Date Relationship Date of Birth   Age Gender  \\\n",
       "0 2024-01-19 23:59:59          Son           NaN   9.0      M   \n",
       "1 2024-01-19 23:59:59     Daughter           NaN   7.0      F   \n",
       "2 2024-01-19 23:59:59       Spouse    19/02/1983  40.0      F   \n",
       "3 2024-01-19 23:59:59         Self    11/08/1974  49.0      M   \n",
       "4 2024-02-12 23:59:59       Spouse    03/02/1991  32.0      M   \n",
       "\n",
       "   Annual Premium without ST /GST  GHI /GPA Sum Insured  CGST 9 %  SGST 9 %  \\\n",
       "0                            0.00                   0.0      0.00      0.00   \n",
       "1                            0.00                   0.0      0.00      0.00   \n",
       "2                            0.00                   0.0      0.00      0.00   \n",
       "3                         6414.54                   NaN    577.31    577.31   \n",
       "4                            0.00                   0.0      0.00      0.00   \n",
       "\n",
       "   IGST 18%  Total Charged Premium including ST /GST  \\\n",
       "0       0.0                                     0.00   \n",
       "1       0.0                                     0.00   \n",
       "2       0.0                                     0.00   \n",
       "3       NaN                                  7569.15   \n",
       "4       0.0                                     0.00   \n",
       "\n",
       "  Status  (Active /Inactive)    Category  Application Number entry 1  \\\n",
       "0                  CANCELLED  CATEGORY 2                      101479   \n",
       "1                  CANCELLED  CATEGORY 2                      101479   \n",
       "2                  CANCELLED  CATEGORY 2                      101479   \n",
       "3                  CANCELLED  CATEGORY 2                      101479   \n",
       "4                  CANCELLED  CATEGORY 2                      101389   \n",
       "\n",
       "   Partner Reference Number 2  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3                         NaN  \n",
       "4                         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns missing in 'MD_RN_UPGRADE_ABHIPROD_2-81-23-0001567-000_1723788251264.xlsx': ['Employee ID', 'Name', 'DOB', 'Coverage Start Date', 'Sum Insured', 'UHID', 'Active', 'Phone', 'Email'].\n",
      "\n",
      "Available columns:\n",
      "\n",
      "|| Master Policy Number   || Combi Reference Number   || Group Policy Number   || Group Policy Holder name   || Product Code   || GPA Rater Plan   || FAMILY_POLICY_NUMBER   || Insured Member Name   || Policy Start Date   || Policy End Date   || ABHI Client ID   || Endorsement Reference Number   || Employee Number   || Member Entry Date   || Member Exit Date   || Relationship   || Date of Birth   || Age   || Gender   || Annual Premium without ST /GST   || GHI /GPA Sum Insured   || CGST 9 %   || SGST 9 %   || IGST 18%   || Total Charged Premium including ST /GST   || Status  (Active /Inactive)   || Category   || Application Number entry 1   || Partner Reference Number 2   ||\n",
      "\n",
      "\n",
      "Please input the column name for 'Employee ID': Employee Number\n",
      "Please input the column name for 'Name': Insured Member Name\n",
      "Please input the column name for 'DOB':  Date of Birth \n",
      "Please input the column name for 'Coverage Start Date': Policy Start Date\n",
      "Please input the column name for 'Sum Insured': \n",
      "Invalid column name. Please try again.\n",
      "Please input the column name for 'Sum Insured': \n",
      "Invalid column name. Please try again.\n",
      "Skipping renaming for missing column 'Sum Insured' after 2 invalid attempts.\n",
      "Please input the column name for 'UHID': \n",
      "Invalid column name. Please try again.\n",
      "Please input the column name for 'UHID': \n",
      "Invalid column name. Please try again.\n",
      "Skipping renaming for missing column 'UHID' after 2 invalid attempts.\n",
      "Please input the column name for 'Active': \n",
      "Invalid column name. Please try again.\n",
      "Please input the column name for 'Active': \n",
      "Invalid column name. Please try again.\n",
      "Skipping renaming for missing column 'Active' after 2 invalid attempts.\n",
      "Please input the column name for 'Phone': \n",
      "Invalid column name. Please try again.\n",
      "Please input the column name for 'Phone': \n",
      "Invalid column name. Please try again.\n",
      "Skipping renaming for missing column 'Phone' after 2 invalid attempts.\n",
      "Please input the column name for 'Email': \n",
      "Invalid column name. Please try again.\n",
      "Please input the column name for 'Email': \n",
      "Invalid column name. Please try again.\n",
      "Skipping renaming for missing column 'Email' after 2 invalid attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loop\\AppData\\Local\\Temp\\ipykernel_4364\\3653295848.py:195: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['DOB'] = pd.to_datetime(df['DOB']).dt.strftime('%d/%b/%Y')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched_main_data is empty. No operations to perform.\n",
      "Recon Data saved to C:\\Users\\Loop\\Desktop\\Process Tools\\Endo Recon Tool\\Output\\Recon Data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "from dateutil import parser\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# Suppress the SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "# Define paths\n",
    "main_path = r'C:\\Users\\Loop\\Desktop\\Process Tools\\Endo Recon Tool\\Input\\Main'\n",
    "sub_path = r'C:\\Users\\Loop\\Desktop\\Process Tools\\Endo Recon Tool\\Input\\Sub'\n",
    "output_path = r'C:\\Users\\Loop\\Desktop\\Process Tools\\Endo Recon Tool\\Output'\n",
    "\n",
    "# Helper function to read files from a given path\n",
    "def read_files_from_path(path):\n",
    "    data_frames = []\n",
    "    for file_name in os.listdir(path):\n",
    "        if file_name.endswith('.xlsx') or file_name.endswith('.xls'):\n",
    "            df = pd.read_excel(os.path.join(path, file_name), None)  # Read all sheets\n",
    "            data_frames.append((file_name, df))\n",
    "        elif file_name.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(path, file_name))\n",
    "            data_frames.append((file_name, df))\n",
    "    return data_frames\n",
    "\n",
    "# Read files from main and sub paths\n",
    "main_data_frames = read_files_from_path(main_path)\n",
    "sub_data_frames = read_files_from_path(sub_path)\n",
    "\n",
    "# Check if any files were found in both paths\n",
    "if not main_data_frames and not sub_data_frames:\n",
    "    print(\"No files found in both Main and Sub directories. Process aborted.\")\n",
    "elif not main_data_frames:\n",
    "    print(\"No files found in Main directory. Process aborted.\")\n",
    "elif not sub_data_frames:\n",
    "    print(\"No files found in Sub directory. Process aborted.\")\n",
    "else:\n",
    "    def process_data_frames(data_frames, data_type=\"main\"):\n",
    "        selected_data = {}\n",
    "    \n",
    "        for file_name, dfs in data_frames:\n",
    "            if isinstance(dfs, dict):  # Check if it's a dictionary of sheets\n",
    "                sheet_names = list(dfs.keys())\n",
    "                if len(sheet_names) > 1:\n",
    "                    print(f\"File '{file_name}' contains multiple sheets: {sheet_names}\")\n",
    "                    selected_sheet = input(f\"Please enter the sheet name you want to process from '{file_name}': \")\n",
    "                    if selected_sheet in sheet_names:\n",
    "                        df = dfs[selected_sheet]\n",
    "                        selected_data[file_name] = df\n",
    "                        print(f\"Processing sheet '{selected_sheet}' from file '{file_name}'\")\n",
    "                    else:\n",
    "                        print(f\"Sheet '{selected_sheet}' not found in file '{file_name}'. Process aborted.\")\n",
    "                else:\n",
    "                    df = dfs[sheet_names[0]]  # Only one sheet, no need to ask\n",
    "                    selected_data[file_name] = df\n",
    "                    print(f\"Processing single sheet '{sheet_names[0]}' from file '{file_name}'\")\n",
    "            else:\n",
    "                df = dfs  # It's a CSV file\n",
    "                selected_data[file_name] = df\n",
    "                print(f\"Processing CSV file '{file_name}'\")\n",
    "\n",
    "        for file_name, df in selected_data.items():\n",
    "            required_columns = [\"Employee ID\", \"Name\", \"Relationship\", \"DOB\", \"Gender\", \"Coverage Start Date\", \"Sum Insured\", \"UHID\", \"Active\",\"Phone\",\"Email\"]\n",
    "            df.columns = [col.strip() for col in df.columns]  # Strip any extra spaces\n",
    "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "            if missing_columns:\n",
    "                # Printing DataFrame in Tabular format\n",
    "                pd.set_option(\"display.max_columns\", None)\n",
    "                display(df.head())\n",
    "\n",
    "                # Print missing and available columns using tabulate in JIRA format\n",
    "                print(f\"\\nColumns missing in '{file_name}': {missing_columns}.\\n\\nAvailable columns:\\n\")\n",
    "                print(tabulate([df.columns], headers='firstrow', tablefmt='jira'))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                for missing in missing_columns:\n",
    "                    attempt = 0\n",
    "                    while attempt < 2:\n",
    "                        column_name = input(f\"Please input the column name for '{missing}': \").strip()\n",
    "                        if column_name in df.columns:\n",
    "                            df.rename(columns={column_name: missing}, inplace=True)\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid column name. Please try again.\")\n",
    "                            attempt += 1\n",
    "                    if attempt == 2:\n",
    "                        print(f\"Skipping renaming for missing column '{missing}' after 2 invalid attempts.\")\n",
    "            else:\n",
    "                print(f\"All required columns are present in file '{file_name}'.\")\n",
    "\n",
    "        return selected_data\n",
    "    \n",
    "    \n",
    "    def clean_and_validate_data(selected_data):\n",
    "        # Function to convert date strings to the desired format\n",
    "        def convert_to_dd_mmm_yyyy(date_str):\n",
    "            if pd.isnull(date_str):\n",
    "                return None\n",
    "            try:\n",
    "                # Parse the date using dateutil.parser\n",
    "                date = parser.parse(date_str)\n",
    "                # Format the date to 'DD-MMM-YYYY'\n",
    "                return date.strftime('%d/%b/%Y')\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing date: {date_str}. Exception: {e}\")\n",
    "                return None  # or handle the exception as needed\n",
    "\n",
    "        def update_relationship(row):\n",
    "            relationship = row['Relationship'].strip().title()  # Remove leading and trailing spaces\n",
    "            gender = row['Gender'].strip().lower()\n",
    "            if relationship in ['Self', 'Employee']:\n",
    "                return 'Self'\n",
    "            elif relationship in ['Parent', 'Mother', 'Father']:\n",
    "                return 'Father' if gender == 'male' else 'Mother'\n",
    "            elif relationship in ['Spouse', 'Wife', 'Husband']:\n",
    "                return 'Husband' if gender == 'male' else 'Wife'\n",
    "            elif relationship in ['Child', 'Son', 'Daughter']:\n",
    "                return 'Son' if gender == 'male' else 'Daughter'\n",
    "            return relationship\n",
    "        \n",
    "        def preprocess_mobile_number(number):\n",
    "            if pd.isna(number):\n",
    "                return number\n",
    "            str_number = str(number).strip()  # Convert to string and strip any surrounding whitespace\n",
    "            if str_number.endswith('.0'):\n",
    "                str_number = str_number[:-2]  # Remove trailing '.0' if present\n",
    "            elif str_number.startswith('+91'):\n",
    "                str_number = str_number[3:]\n",
    "        \n",
    "            # Remove any internal spaces and return the number if it only contains digits\n",
    "            str_number = str_number.replace(' ', '')  # Remove any spaces within the number\n",
    "\n",
    "            return str_number if str_number.isdigit() else number  # Return as string if it's a valid number\n",
    "        \n",
    "        \n",
    "        # Convert 'Relation' column values\n",
    "        def convert_relation(relation):\n",
    "            if relation == 'Employee':\n",
    "                 return 'Self'\n",
    "            elif relation in ['Wife', 'Husband']:\n",
    "                 return 'Spouse'\n",
    "            elif relation in ['Daughter', 'Son']:\n",
    "                return 'Child'\n",
    "            elif relation in ['Father', 'Mother']:\n",
    "                 return 'Parent'\n",
    "            elif relation in ['Father In Law', 'Mother In Law']:\n",
    "                 return 'Parent-in-law'\n",
    "            else:\n",
    "                return relation\n",
    "            \n",
    "            \n",
    "        def update_gender(row):\n",
    "            Gender = row['Gender'].strip().title()  # Remove leading and trailing spaces\n",
    "            if Gender in ['Male', 'M']:\n",
    "                return 'Male'\n",
    "            elif Gender in ['Female', 'F']:\n",
    "                return 'Female' \n",
    "            return Gender\n",
    "\n",
    "\n",
    "        # Process the data in each file\n",
    "        for file_name, df in selected_data.items():\n",
    "            if df['Employee ID'].dtype == 'object':\n",
    "                df['Employee ID'] = df['Employee ID'].str.replace(r'\\s+', '', regex=True)\n",
    "            else:\n",
    "                # Handle NaN or inf values before converting to int\n",
    "                df['Employee ID'] = df['Employee ID'].replace([np.inf, -np.inf], np.nan).fillna(0).astype(int)\n",
    "\n",
    "            if 'Name' in df.columns:\n",
    "                df['Name'] = df['Name'].fillna('')\n",
    "                df['Name'] = df['Name'].astype(str).str.lower()\n",
    "                titles_to_remove = ['mr', 'mrs', 'master', 'dr', 'miss']\n",
    "                title_pattern = r'\\b(?:' + '|'.join(titles_to_remove) + r')\\b'\n",
    "                special_chars_pattern = r'[!@#$%^&*()_+\\-=\\[\\]{}\\\\|:\";\\'<>?,./1234567890]'\n",
    "                df['Name'] = (df['Name']\n",
    "                              .str.replace(title_pattern, '', regex=True)\n",
    "                              .str.replace(special_chars_pattern, '', regex=True)\n",
    "                              .str.replace(r'\\s+', ' ', regex=True)\n",
    "                              .str.strip()\n",
    "                              .str.title())\n",
    "                df['Name'] = df['Name'].replace('', np.nan)\n",
    "\n",
    "            if 'Gender' in df.columns:\n",
    "                df['Gender'] = df.apply(update_gender, axis=1)\n",
    "\n",
    "            if 'DOB' in df.columns:\n",
    "                df['DOB'] = pd.to_datetime(df['DOB']).dt.strftime('%d/%b/%Y')\n",
    "                df['DOB'] = df['DOB'].apply(lambda x: convert_to_dd_mmm_yyyy(x) if pd.notnull(x) else None)\n",
    "                df['DOB'] = df['DOB'].astype(str).str.strip().str.replace(r'[\\u200B-\\u200D\\uFEFF]', '', regex=True)\n",
    "\n",
    "            if 'DOB' in df.columns:\n",
    "                df['Age'] = pd.to_datetime(df['DOB'], format='mixed', dayfirst=True, errors='coerce')\n",
    "                df['Age'] = ((datetime.now() - df['Age']).dt.days // 365).fillna(0).astype(int)\n",
    "                cols = df.columns.tolist()\n",
    "                dob_index = cols.index('DOB')\n",
    "                cols.insert(dob_index + 1, cols.pop(cols.index('Age')))\n",
    "                df = df[cols]\n",
    "\n",
    "            if 'Relationship' in df.columns:\n",
    "                df['Relationship'] = df.apply(update_relationship, axis=1)\n",
    "         \n",
    "            # Reorder columns to have 'Relationship' next to 'Name'\n",
    "            if 'Relationship' in df.columns:\n",
    "                cols = df.columns.tolist()\n",
    "                relation_index = cols.index('Name')\n",
    "                cols.insert(relation_index + 1, cols.pop(cols.index('Relationship')))\n",
    "                df = df[cols]\n",
    "                              \n",
    "\n",
    "            df['Relation Type'] = df['Relationship'].apply(convert_relation)\n",
    "    \n",
    "            # Reorder columns to have 'Relation Type' next to 'Relation'\n",
    "            cols = df.columns.tolist()\n",
    "            relation_index = cols.index('Relationship')\n",
    "            cols.insert(relation_index + 1, cols.pop(cols.index('Relation Type')))\n",
    "            df = df[cols]\n",
    "\n",
    "\n",
    "            if 'Coverage Start Date' in df.columns:\n",
    "                df['Coverage Start Date'] = df['Coverage Start Date'].astype(str).str.strip().apply(lambda x: convert_to_dd_mmm_yyyy(x) if x else '')\n",
    "\n",
    "            if 'Sum Insured' in df.columns:\n",
    "                # Remove whitespace and ensure the value is a string without float formatting\n",
    "                df['Sum Insured'] = df['Sum Insured'].astype(str).str.replace(r'\\s+', '', regex=True).str.split('.').str[0]\n",
    "\n",
    "\n",
    "            if 'Employee ID' in df.columns and 'Relationship' in df.columns and 'Coverage Start Date' in df.columns:\n",
    "                for emp_id in df.loc[df['Relationship'].isin(['Self', 'Employee']), 'Employee ID'].unique():\n",
    "                    coverage_start_date = df.loc[(df['Employee ID'] == emp_id) & (df['Relationship'] == 'Self'), 'Coverage Start Date'].iloc[0]\n",
    "                    df.loc[df['Employee ID'] == emp_id, 'Coverage Start Date'] = coverage_start_date\n",
    "\n",
    "            if 'Employee ID' in df.columns and 'Relationship' in df.columns and 'Sum Insured' in df.columns:\n",
    "                for emp_id in df.loc[df['Relationship'].isin(['Self', 'Employee']), 'Employee ID'].unique():\n",
    "                    sum_insured = df.loc[(df['Employee ID'] == emp_id) & (df['Relationship'] == 'Self'), 'Sum Insured'].iloc[0]\n",
    "                    df.loc[df['Employee ID'] == emp_id, 'Sum Insured'] = sum_insured\n",
    "                    \n",
    "            # Assuming df is your DataFrame\n",
    "            if 'UHID' in df.columns:\n",
    "                df['UHID'] = df['UHID'].astype(str).str.replace(r'\\s+', '', regex=True).str.split('.').str[0]\n",
    "                df['UHID'] = df['UHID'].replace(['nan', ''], '')\n",
    "                \n",
    "                \n",
    "            # Mapping dictionary\n",
    "            mapping = {\n",
    "                'Yes': 'Yes', 'Y': 'Yes', 'Active': 'Yes', 'A': 'Yes', 'M': 'Yes', 'ACTIVE': 'Yes', 'YES': 'Yes',\n",
    "                'No': 'No', 'N': 'No', 'In-Active': 'No', 'In active': 'No', 'Delete': 'No', 'D': 'No', \n",
    "                'Deleted/Cancelled': 'No', 'IN-ACTIVE': 'No', 'DELETE': 'No', 'NO': 'No', 'CANCELLED': 'No'\n",
    "            }\n",
    "\n",
    "            # Check if 'Active' column exists\n",
    "            if 'Active' in df.columns:\n",
    "                df['Active'] = df['Active'].astype(str).str.strip().replace('', np.nan)\n",
    "                df['Active'] = df['Active'].map(mapping).fillna(df['Active'])\n",
    "            else:\n",
    "                df['Active'] = np.nan\n",
    "                            \n",
    "\n",
    "            # Apply preprocessing to the 'Phone' column\n",
    "            if 'Phone' in df.columns:\n",
    "                df['Phone'] = df['Phone'].apply(preprocess_mobile_number)\n",
    "                \n",
    "            # Clean the 'Email' column\n",
    "            if 'Email' in df.columns:\n",
    "                \n",
    "                if not df['Email'].isna().all():\n",
    "                    df['Email'] = df['Email'].str.lower()\n",
    "\n",
    "                 \n",
    "            selected_data[file_name] = df\n",
    "        return selected_data\n",
    "    \n",
    "    \n",
    "    def compare_and_filter_employee_ids(selected_main_data, selected_sub_data):\n",
    "        main_employee_ids = set()\n",
    "        sub_employee_ids = set()\n",
    "\n",
    "        # Collect all Employee IDs from main data\n",
    "        for file_name, df in selected_main_data.items():\n",
    "            if 'Employee ID' in df.columns:\n",
    "                main_employee_ids.update(df['Employee ID'].unique())\n",
    "\n",
    "        # Collect all Employee IDs from sub data\n",
    "        for file_name, df in selected_sub_data.items():\n",
    "            if 'Employee ID' in df.columns:\n",
    "                sub_employee_ids.update(df['Employee ID'].unique())\n",
    "\n",
    "        # Identify mismatches\n",
    "        sub_not_in_main = sub_employee_ids - main_employee_ids\n",
    "\n",
    "        # Create unmatch_sub_data and update selected_sub_data\n",
    "        unmatch_sub_data = {}\n",
    "        for file_name, df in selected_sub_data.items():\n",
    "            if 'Employee ID' in df.columns:\n",
    "                # Identify unmatched rows\n",
    "                unmatched_rows = df[df['Employee ID'].isin(sub_not_in_main)]\n",
    "                if not unmatched_rows.empty:\n",
    "                    unmatch_sub_data[file_name] = unmatched_rows\n",
    "            \n",
    "                # Update selected_sub_data to keep only matching rows\n",
    "                matched_rows = df[~df['Employee ID'].isin(sub_not_in_main)]\n",
    "                selected_sub_data[file_name] = matched_rows\n",
    "\n",
    "        return unmatch_sub_data, selected_sub_data\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------------------------------    \n",
    "       \n",
    "    \n",
    "    # Function to save data to Excel\n",
    "    def save_to_excel(selected_main_data, Unmatched_main_data, selected_sub_data, unmatch_sub_data,  output_path):\n",
    "        output_file = os.path.join(output_path, 'Recon Data.xlsx')\n",
    "    \n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            # Save selected_main_data to \"Main Data\" sheet\n",
    "            sheet_name_main = 'Main Data'\n",
    "            for file_name, df in selected_main_data.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name_main, index=False)\n",
    "                \n",
    "            # Save Unmatched_main_data to \"Unmatched Main Data\" sheet\n",
    "            sheet_name_unmatched_main = 'Unmatched Main Data'\n",
    "            for file_name, df in Unmatched_main_data.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name_unmatched_main, index=False)\n",
    "                \n",
    "             # Save unmatch_sub_data to \"Unmatched Sub Data\" sheet\n",
    "            sheet_name_matched = 'selected Sub Data'\n",
    "            for file_name, df in selected_sub_data.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name_matched, index=False)\n",
    "        \n",
    "            # Save unmatch_sub_data to \"Unmatched Sub Data\" sheet\n",
    "            sheet_name_unmatched = 'Unmatched Sub Data'\n",
    "            for file_name, df in unmatch_sub_data.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name_unmatched, index=False)\n",
    "                \n",
    "\n",
    "        print(f\"Recon Data saved to {output_file}\")\n",
    "\n",
    "    # Process main data frames\n",
    "    selected_main_data = process_data_frames(main_data_frames, data_type=\"main\")\n",
    "\n",
    "    # Process sub data frames\n",
    "    selected_sub_data = process_data_frames(sub_data_frames, data_type=\"sub\")\n",
    "\n",
    "    # Clean and validate main data\n",
    "    selected_main_data = clean_and_validate_data(selected_main_data)\n",
    "\n",
    "    # Clean and validate sub data\n",
    "    selected_sub_data = clean_and_validate_data(selected_sub_data)\n",
    "\n",
    "    # Compare Employee IDs between main and sub data and filter sub data\n",
    "    unmatch_sub_data, selected_sub_data = compare_and_filter_employee_ids(selected_main_data, selected_sub_data)\n",
    "\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "   # Create a temporary column by concatenating 'Employee ID' (converted to string) and 'Name'\n",
    "    for file_name, df in selected_main_data.items():\n",
    "        if 'Employee ID' in df.columns and 'Name' in df.columns:\n",
    "            df['Temp_Column'] = df['Employee ID'].astype(str).str.replace(r'[^a-zA-Z0-9]', '', regex=True).str.upper() + \" \" + df['Name']\n",
    "    \n",
    "        # To fill any blank value in df['Temp_Column']\n",
    "        df['Temp_Column'] = df['Temp_Column'].fillna('Unknown')\n",
    "    \n",
    "    # Create a temporary column by concatenating 'Employee ID' and 'Name' for sub data\n",
    "    for file_name, comparison_df in selected_sub_data.items():\n",
    "        if 'Employee ID' in comparison_df.columns and 'Name' in comparison_df.columns:\n",
    "            comparison_df['Temp_Column'] = comparison_df['Employee ID'].astype(str).str.replace(r'[^a-zA-Z0-9]', '', regex=True).str.upper() + \" \" + comparison_df['Name']\n",
    "        \n",
    "        # To fill any blank value in df['Temp_Column']\n",
    "        comparison_df['Temp_Column'] = comparison_df['Temp_Column'].fillna('Unknown')\n",
    "    \n",
    "    \n",
    "    # Getting User ID and UHID column from Genome data\n",
    "    # Columns \n",
    "    Relationship_column = \"Relationship\"\n",
    "    DOB_column = \"DOB\"\n",
    "    Gender_column = \"Gender\"\n",
    "    Sum_Insured_column = \"Sum Insured\"\n",
    "    Coverage_Start_Date_column = \"Coverage Start Date\"\n",
    "    UHID_column = \"UHID\"\n",
    "    active_column = 'Active'\n",
    "    phone_column = 'Phone'\n",
    "    email_column = 'Email'\n",
    "\n",
    "\n",
    "    # Check if the columns exist in the comparison_df\n",
    "    Relationship_exists = Relationship_column in comparison_df.columns\n",
    "    DOB_exists = DOB_column in comparison_df.columns\n",
    "    Gender_exists = Gender_column in comparison_df.columns\n",
    "    Sum_Insured_exists = Sum_Insured_column in comparison_df.columns\n",
    "    Coverage_Start_Date_exists = Coverage_Start_Date_column in comparison_df.columns\n",
    "    uhid_exists = UHID_column in comparison_df.columns\n",
    "    active_exists = active_column in comparison_df.columns\n",
    "    phone_exists = phone_column in comparison_df.columns\n",
    "    email_exists = email_column in comparison_df.columns\n",
    "\n",
    "\n",
    "    def match_names(df_temp, comp_temp, comp_uhid=None, comp_active=None, comp_Relationship=None, comp_DOB=None, comp_Gender=None, comp_Sum_Insured=None, comp_Coverage_Start_Date=None, comp_phone=None, comp_email=None):\n",
    "        \n",
    "        df_parts = df_temp.split()\n",
    "        comp_parts = comp_temp.split()\n",
    "\n",
    "        # Check if Employee ID matches\n",
    "        if df_parts[:2] != comp_parts[:2]:\n",
    "            return False, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "        # Compare the names\n",
    "        df_name_parts = set(df_parts[2:])\n",
    "        comp_name_parts = set(comp_parts[2:])\n",
    "\n",
    "        # Finding common name parts\n",
    "        common_parts = df_name_parts.intersection(comp_name_parts)\n",
    "\n",
    "        # We could also consider common_parts >= 1 if we want to be less strict\n",
    "        if len(common_parts) >= 1:\n",
    "            return True, comp_Relationship, comp_DOB, comp_Gender, comp_Sum_Insured, comp_Coverage_Start_Date, comp_uhid, comp_active, comp_phone, comp_email\n",
    "        else:\n",
    "            return False, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "    def get_matches(df_temp, comparison_df):\n",
    "        for i, row in comparison_df.iterrows():\n",
    "            match, active, Relationship, DOB, Gender, Sum_Insured, Coverage_Start_Date, UHID, phone, email,  = match_names(\n",
    "                df_temp,\n",
    "                row['Temp_Column'],\n",
    "                row[UHID_column] if UHID_column in comparison_df.columns else None,\n",
    "                row[active_column] if active_column in comparison_df.columns else None,\n",
    "                row[Relationship_column] if Relationship_column in comparison_df.columns else None,\n",
    "                row[DOB_column] if DOB_column in comparison_df.columns else None,\n",
    "                row[Gender_column] if Gender_column in comparison_df.columns else None,\n",
    "                row[Sum_Insured_column] if Sum_Insured_column in comparison_df.columns else None,\n",
    "                row[Coverage_Start_Date_column] if Coverage_Start_Date_column in comparison_df.columns else None,\n",
    "                row[phone_column] if phone_column in comparison_df.columns else None,\n",
    "                row[email_column] if email_column in comparison_df.columns else None\n",
    "            )\n",
    "            if match:\n",
    "                return row['Temp_Column']\n",
    "        return None\n",
    "\n",
    "    # Apply the matching function\n",
    "    df['Match Found Genome'] = df['Temp_Column'].apply(\n",
    "        lambda x: any(\n",
    "            match_names(\n",
    "                x,\n",
    "                row['Temp_Column'],\n",
    "                row[UHID_column] if UHID_column in comparison_df.columns else None,\n",
    "                row[active_column] if active_column in comparison_df.columns else None,\n",
    "                row[Relationship_column] if Relationship_column in comparison_df.columns else None,\n",
    "                row[DOB_column] if DOB_column in comparison_df.columns else None,\n",
    "                row[Gender_column] if Gender_column in comparison_df.columns else None,\n",
    "                row[Sum_Insured_column] if Sum_Insured_column in comparison_df.columns else None,\n",
    "                row[Coverage_Start_Date_column] if Coverage_Start_Date_column in comparison_df.columns else None,\n",
    "                row[phone_column] if phone_column in comparison_df.columns else None,\n",
    "                row[email_column] if email_column in comparison_df.columns else None\n",
    "            )[0] for _, row in comparison_df.iterrows()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extra added Start\n",
    "    df['Found on Genome'] = df['Temp_Column'].apply(\n",
    "        lambda x: get_matches(x, comparison_df) if any(\n",
    "            match_names(\n",
    "                x,\n",
    "                row['Temp_Column'],\n",
    "                row[UHID_column] if UHID_column in comparison_df.columns else None,\n",
    "                row[active_column] if active_column in comparison_df.columns else None,\n",
    "                row[Relationship_column] if Relationship_column in comparison_df.columns else None,\n",
    "                row[DOB_column] if DOB_column in comparison_df.columns else None,\n",
    "                row[Gender_column] if Gender_column in comparison_df.columns else None,\n",
    "                row[Sum_Insured_column] if Sum_Insured_column in comparison_df.columns else None,\n",
    "                row[Coverage_Start_Date_column] if Coverage_Start_Date_column in comparison_df.columns else None,\n",
    "                row[phone_column] if phone_column in comparison_df.columns else None,\n",
    "                row[email_column] if email_column in comparison_df.columns else None\n",
    "            )[0] for _, row in comparison_df.iterrows()\n",
    "        ) else None\n",
    "    )\n",
    "\n",
    "    # Apply the matching function and retrieve UHID, Active status, and other specified columns\n",
    "    def get_matching_info(temp_value):\n",
    "        for i, row in comparison_df.iterrows():\n",
    "            match, Relationship, DOB, Gender, Sum_Insured, Coverage_Start_Date, UHID, active, phone, email = match_names(\n",
    "                temp_value,\n",
    "                row['Temp_Column'],\n",
    "                row[UHID_column] if UHID_column in comparison_df.columns else None,\n",
    "                row[active_column] if active_column in comparison_df.columns else None,\n",
    "                row[Relationship_column] if Relationship_column in comparison_df.columns else None,\n",
    "                row[DOB_column] if DOB_column in comparison_df.columns else None,\n",
    "                row[Gender_column] if Gender_column in comparison_df.columns else None,\n",
    "                row[Sum_Insured_column] if Sum_Insured_column in comparison_df.columns else None,\n",
    "                row[Coverage_Start_Date_column] if Coverage_Start_Date_column in comparison_df.columns else None,\n",
    "                row[phone_column] if phone_column in comparison_df.columns else None,\n",
    "                row[email_column] if email_column in comparison_df.columns else None\n",
    "            )\n",
    "            if match:\n",
    "                return Relationship, DOB, Gender, Sum_Insured, Coverage_Start_Date, UHID, active, phone, email\n",
    "        return None, None, None, None, None, None, None, None, None\n",
    "\n",
    "    df['Sub Relationship'], df['Sub DOB'], df['Sub Gender'], df['Sub Sum Insured'], df['Sub Coverage Start Date'], df['Sub UHID'], df['Sub Active'], df['Sub Phone'], df['Sub Email'] = zip(*df['Temp_Column'].map(get_matching_info))\n",
    "    \n",
    "    #-------------------------------------------Code for Unmatched columns-----------------------------------------------\n",
    "    \n",
    "    # Filter unmatched data and store it in a dictionary format\n",
    "    Unmatched_main_data = {file_name: df[df['Match Found Genome'] == False].copy() for file_name, df in selected_main_data.items()}\n",
    "\n",
    "    # Remove the unmatched rows from selected_main_data\n",
    "    selected_main_data = {file_name: df[df['Match Found Genome'] == True] for file_name, df in selected_main_data.items()}\n",
    "    \n",
    "    # Columns to be removed\n",
    "    columns_to_remove = [\n",
    "        'Temp_Column', 'Match Found Genome', 'Found on Genome',\n",
    "        'Sub Relationship', 'Sub DOB', 'Sub Gender', 'Sub Sum Insured',\n",
    "        'Sub Coverage Start Date', 'Sub UHID', 'Sub Active', 'Sub Phone','Sub Email'\n",
    "    ]\n",
    "\n",
    "    # Remove the specified columns from each DataFrame in Unmatched_main_data\n",
    "    for file_name, df in Unmatched_main_data.items():\n",
    "        df.drop(columns=columns_to_remove, inplace=True)\n",
    "        \n",
    "        \n",
    "    # Create a temporary column by concatenating 'Employee ID' (converted to string) and 'Name'\n",
    "    for file_name, df in Unmatched_main_data.items():\n",
    "        if 'Employee ID' in df.columns and 'DOB' in df.columns and 'Name' in df.columns:\n",
    "            df['Temp_Column1'] = df['Employee ID'].astype(str).str.replace(r'[^a-zA-Z0-9]', '', regex=True).str.upper() + \" \" + df['DOB'] + \" \" + df['Name']\n",
    "    \n",
    "        # To fill any blank value in df['Temp_Column']\n",
    "        df['Temp_Column1'] = df['Temp_Column1'].fillna('Unknown')\n",
    "    \n",
    "    # Create a temporary column by concatenating 'Employee ID' and 'Name' for sub data\n",
    "    for file_name, comparison_df in selected_sub_data.items():\n",
    "        if 'Employee ID' in comparison_df.columns and 'Name' in comparison_df.columns:\n",
    "            comparison_df['Temp_Column1'] = comparison_df['Employee ID'].astype(str).str.replace(r'[^a-zA-Z0-9]', '', regex=True).str.upper() + \" \" + comparison_df['DOB'] + \" \" + comparison_df['Name']\n",
    "        \n",
    "        # To fill any blank value in df['Temp_Column']\n",
    "        comparison_df['Temp_Column1'] = comparison_df['Temp_Column1'].fillna('Unknown')\n",
    "        \n",
    "        \n",
    "    def match_names(df_temp, comp_temp, comp_uhid=None, comp_active=None, comp_Relationship=None, comp_DOB=None, comp_Gender=None, comp_Sum_Insured=None, comp_Coverage_Start_Date=None, comp_phone=None, comp_email=None):\n",
    "        df_parts = df_temp.split()\n",
    "        comp_parts = comp_temp.split()\n",
    "\n",
    "        # Check if Employee ID matches\n",
    "        if df_parts[:2] != comp_parts[:2]:\n",
    "            return False, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "        # Compare the names\n",
    "        df_name_parts = set(df_parts[2:])\n",
    "        comp_name_parts = set(comp_parts[2:])\n",
    "\n",
    "        # Finding common name parts\n",
    "        common_parts = df_name_parts.intersection(comp_name_parts)\n",
    "\n",
    "        # We could also consider common_parts >= 1 if we want to be less strict\n",
    "        if len(common_parts) == 1:\n",
    "            return True, comp_Relationship, comp_DOB, comp_Gender, comp_Sum_Insured, comp_Coverage_Start_Date, comp_uhid, comp_active, comp_phone, comp_email\n",
    "        else:\n",
    "            return False, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "    def get_matches(df_temp, comparison_df):\n",
    "        for i, row in comparison_df.iterrows():\n",
    "            match, active, Relationship, DOB, Gender, Sum_Insured, Coverage_Start_Date, UHID, phone, email = match_names(\n",
    "                df_temp,\n",
    "                row['Temp_Column1'],\n",
    "                row[UHID_column] if UHID_column in comparison_df.columns else None,\n",
    "                row[active_column] if active_column in comparison_df.columns else None,\n",
    "                row[Relationship_column] if Relationship_column in comparison_df.columns else None,\n",
    "                row[DOB_column] if DOB_column in comparison_df.columns else None,\n",
    "                row[Gender_column] if Gender_column in comparison_df.columns else None,\n",
    "                row[Sum_Insured_column] if Sum_Insured_column in comparison_df.columns else None,\n",
    "                row[Coverage_Start_Date_column] if Coverage_Start_Date_column in comparison_df.columns else None,\n",
    "                row[phone_column] if phone_column in comparison_df.columns else None,\n",
    "                row[email_column] if email_column in comparison_df.columns else None\n",
    "            )\n",
    "            if match:\n",
    "                return row['Temp_Column1']\n",
    "        return None\n",
    "\n",
    "    # Apply the matching function\n",
    "    df['Match Found Genome'] = df['Temp_Column1'].apply(\n",
    "        lambda x: any(\n",
    "            match_names(\n",
    "                x,\n",
    "                row['Temp_Column1'],\n",
    "                row[UHID_column] if UHID_column in comparison_df.columns else None,\n",
    "                row[active_column] if active_column in comparison_df.columns else None,\n",
    "                row[Relationship_column] if Relationship_column in comparison_df.columns else None,\n",
    "                row[DOB_column] if DOB_column in comparison_df.columns else None,\n",
    "                row[Gender_column] if Gender_column in comparison_df.columns else None,\n",
    "                row[Sum_Insured_column] if Sum_Insured_column in comparison_df.columns else None,\n",
    "                row[Coverage_Start_Date_column] if Coverage_Start_Date_column in comparison_df.columns else None,\n",
    "                row[phone_column] if phone_column in comparison_df.columns else None,\n",
    "                row[email_column] if email_column in comparison_df.columns else None\n",
    "            )[0] for _, row in comparison_df.iterrows()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extra added Start\n",
    "    df['Found on Genome'] = df['Temp_Column1'].apply(\n",
    "        lambda x: get_matches(x, comparison_df) if any(\n",
    "            match_names(\n",
    "                x,\n",
    "                row['Temp_Column1'],\n",
    "                row[UHID_column] if UHID_column in comparison_df.columns else None,\n",
    "                row[active_column] if active_column in comparison_df.columns else None,\n",
    "                row[Relationship_column] if Relationship_column in comparison_df.columns else None,\n",
    "                row[DOB_column] if DOB_column in comparison_df.columns else None,\n",
    "                row[Gender_column] if Gender_column in comparison_df.columns else None,\n",
    "                row[Sum_Insured_column] if Sum_Insured_column in comparison_df.columns else None,\n",
    "                row[Coverage_Start_Date_column] if Coverage_Start_Date_column in comparison_df.columns else None,\n",
    "                row[phone_column] if phone_column in comparison_df.columns else None,\n",
    "                row[email_column] if email_column in comparison_df.columns else None\n",
    "            )[0] for _, row in comparison_df.iterrows()\n",
    "        ) else None\n",
    "    )\n",
    "\n",
    "    # Apply the matching function and retrieve UHID, Active status, and other specified columns\n",
    "    def get_matching_info(temp_value):\n",
    "        for i, row in comparison_df.iterrows():\n",
    "            match, Relationship, DOB, Gender, Sum_Insured, Coverage_Start_Date, UHID, active, phone, email = match_names(\n",
    "                temp_value,\n",
    "                row['Temp_Column1'],\n",
    "                row[UHID_column] if UHID_column in comparison_df.columns else None,\n",
    "                row[active_column] if active_column in comparison_df.columns else None,\n",
    "                row[Relationship_column] if Relationship_column in comparison_df.columns else None,\n",
    "                row[DOB_column] if DOB_column in comparison_df.columns else None,\n",
    "                row[Gender_column] if Gender_column in comparison_df.columns else None,\n",
    "                row[Sum_Insured_column] if Sum_Insured_column in comparison_df.columns else None,\n",
    "                row[Coverage_Start_Date_column] if Coverage_Start_Date_column in comparison_df.columns else None,\n",
    "                row[phone_column] if phone_column in comparison_df.columns else None,\n",
    "                row[email_column] if email_column in comparison_df.columns else None\n",
    "            )\n",
    "            if match:\n",
    "                return Relationship, DOB, Gender, Sum_Insured, Coverage_Start_Date, UHID, active, phone, email\n",
    "        return None, None, None, None, None, None, None, None, None\n",
    "\n",
    "    if not df.empty:\n",
    "        df['Sub Relationship'], df['Sub DOB'], df['Sub Gender'], df['Sub Sum Insured'], df['Sub Coverage Start Date'], df['Sub UHID'], df['Sub Active'], df['Sub Phone'], df['Sub Email'] = zip(*df['Temp_Column1'].map(get_matching_info))\n",
    "    else:\n",
    "        print(\"Unmatched_main_data is empty. No operations to perform.\")\n",
    "    \n",
    "    def move_matched_rows(unmatched_data, selected_data):\n",
    "        for file_name, df_unmatched in unmatched_data.items():\n",
    "            matched_rows = df_unmatched[df_unmatched['Match Found Genome'] == True]\n",
    "            for index, row in matched_rows.iterrows():\n",
    "                # Move the row directly without checking if Employee ID is in selected_data\n",
    "                selected_data[file_name] = pd.concat([selected_data[file_name], pd.DataFrame([row])], ignore_index=True)\n",
    "                unmatched_data[file_name].drop(index, inplace=True)\n",
    "\n",
    "            # Sort the dataframes after moving rows\n",
    "            unmatched_data[file_name] = unmatched_data[file_name].sort_values(by=['Employee ID']).reset_index(drop=True)\n",
    "            selected_data[file_name] = selected_data[file_name].sort_values(by=['Employee ID']).reset_index(drop=True)\n",
    "\n",
    "    # Apply the function\n",
    "    move_matched_rows(Unmatched_main_data, selected_main_data)\n",
    "    \n",
    "    # Iterate over items in selected_main_data and create 'Match' column\n",
    "    for file_name, df in selected_main_data.items():\n",
    "        df['Match Relationship'] = df['Relationship'] == df['Sub Relationship']\n",
    "        df['Match Gender'] = df['Gender'] == df['Sub Gender']\n",
    "        df['Match DOB'] = df['DOB'] == df['Sub DOB']\n",
    "        df['Match Sum Insured'] = df['Sum Insured'] == df['Sub Sum Insured']\n",
    "        df['Match UHID'] = df['UHID'] == df['Sub UHID']\n",
    "        df['Match Sub Active'] = df['Active'] == df['Sub Active']\n",
    "        df['Match Sub Phone'] = df['Phone'] == df['Sub Phone']\n",
    "        df['Match Sub Email'] = df['Email'] == df['Sub Email']\n",
    "        \n",
    "\n",
    "    \n",
    "      \n",
    " #-------------------------------------------End of Code for Unmatched columns-----------------------------------------------\n",
    "  \n",
    "    # Save the data to Excel\n",
    "    save_to_excel(selected_main_data, Unmatched_main_data, selected_sub_data, unmatch_sub_data, output_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02b0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
